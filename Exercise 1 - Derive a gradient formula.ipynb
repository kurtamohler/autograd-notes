{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d20e6f0",
   "metadata": {},
   "source": [
    "## Problem description\n",
    "\n",
    "The assignment for the first week is the first part of the [PyTorch Autograd Onboarding Lab](https://github.com/pytorch/pytorch/wiki/Autograd-Onboarding-Lab).\n",
    "\n",
    "We're given a function composed of a few PyTorch function calls, and we have to derive a gradient formula for it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89594b",
   "metadata": {},
   "source": [
    "```python\n",
    "def attn(q, k, v):\n",
    "    x = torch.matmul(q, k.transpose(0, 1))\n",
    "    a = torch.tanh(x)\n",
    "    o = torch.matmul(a, v)\n",
    "    return o, a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40336b1f",
   "metadata": {},
   "source": [
    "## Mario's guidance on solving this\n",
    "\n",
    "\n",
    "> A bit of background more on the Onboarding exercise and some words for you to look up on the internet:\n",
    ">\n",
    "> Start by trying to compute the gradients for the matrix multiplication of two matrices `X = QK^T`, with `Q` and `K` of size (2, 3) .\n",
    ">\n",
    "> What this means is that, given `gX` (a gradient of `X`) of the same shape as X, you should try to compute the gradient wrt. `Q` and wrt. `K`.\n",
    ">\n",
    "> The way to do this is to compute the vector-Jacobian product (aka vjp). To do this, you need to compute the Jacobian matrix of the transformation `Q -> QK^T`. This map takes a matrix of shape `(2, 3)` (i.e. of 6 elements) and transforms it into a matrix of shape `(2, 2)` (4 elements). As such, it can be thought as a map from R^6 to R^4.\n",
    ">\n",
    "> You can then compute the Jacobian `J` of this map, which will be a matrix of shape `(4, 6)`. Then, given the gradient `gX` of shape `(2, 2)` (i.e. of 4 elements), you can flatten it, and compute the vector-Jacobian product, that is, `flatten(gX)*J`. This will give you a vector of size 6. You finally unflatten this vector and that will be the gradient `gQ` of `Q` wrt.  `X` (!!)\n",
    ">\n",
    "> This is certainly a mouthful, but if you do not remember the definition of a Jacobian and so on, you can always go to Wikipedia and look at it there.\n",
    ">\n",
    "> After doing the above, you should have a formula coordinate-by-coordinate of the values of `gQ` in terms of those of `K` and those of `gX`. Something you should think then is \"how can I write this in a simple way, without referring to the coordiantes?\".\n",
    ">\n",
    "> When you have a concise formula for the above, you can try to do the same thing with `X` wrt `K`.\n",
    "Then, you can continue doing the same thing with the other parts of the equation, but actually, starting from the bottom to the top as gradients \"flow backwards\" (even if we do not have a clear understanding of why yet): we start with the gradient of the output and we work our way up to compute the gradients of the intermediary  values, and finish by computing the gradients wrt. the inputs\n",
    "\n",
    "[Wikipedia - Jacobian matrix and determinant](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a06c737e",
   "metadata": {},
   "source": [
    "## Deriving the matrix multiply gradient\n",
    "\n",
    "So we have two matrices `Q` and `K`, both of size `(2, 3)`:\n",
    "\n",
    "```python\n",
    "Q = [[q0, q1, q2],\n",
    "     [q3, q4, q5]]\n",
    "\n",
    "K = [[k0, k1, k2],\n",
    "     [k3, k4, k5]]\n",
    "```\n",
    "\n",
    "Transpose `K` to get:\n",
    "\n",
    "```python\n",
    "K^T = [[k0, k3],\n",
    "       [k1, k4],\n",
    "       [k2, k5]]\n",
    "```\n",
    "\n",
    "The result of the matrix multiply is:\n",
    "\n",
    "```python\n",
    "X = QK^T\n",
    "X = [[x0, x1],\n",
    "     [x2, x3]]\n",
    "X = [[(q0*k0 + q1*k1 + q2*k2), (q0*k3 + q1*k4 + q2*k5)],\n",
    "     [(q3*k0 + q4*k1 + q5*k2), (q3*k3 + q4*k4 + q5*k5)]]\n",
    "\n",
    "```\n",
    "\n",
    "For calculating the gradient, we are given `gX`, which is the same shape as `X`:\n",
    "\n",
    "```python\n",
    "gX = [[gx0, gx1],\n",
    "      [gx2, gx3]]\n",
    "```\n",
    "\n",
    "First, let's find `gQ`, which will be the same size as `Q`. The Jacobian matrix is supposed to be `(4, 6)`. I'll use the definition of the Jacobian matrix from the Wikipedia article. In particular, this one:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We get this:\n",
    "\n",
    "```python\n",
    "J_Q = [[∂x0/∂q0, ∂x0/∂q1, ∂x0/∂q2, ∂x0/∂q3, ∂x0/∂q4, ∂x0/∂q5],\n",
    "       [∂x1/∂q0, ∂x1/∂q1, ∂x1/∂q2, ∂x1/∂q3, ∂x1/∂q4, ∂x1/∂q5],\n",
    "       [∂x2/∂q0, ∂x2/∂q1, ∂x2/∂q2, ∂x2/∂q3, ∂x2/∂q4, ∂x2/∂q5],\n",
    "       [∂x3/∂q0, ∂x3/∂q1, ∂x3/∂q2, ∂x3/∂q3, ∂x3/∂q4, ∂x3/∂q5],\n",
    "```\n",
    "\n",
    "Calculating those partial derivatives and plugging them in, we get:\n",
    "\n",
    "```python\n",
    "J_Q = [[k0, k1, k2,  0,  0,  0],\n",
    "       [k3, k4, k5,  0,  0,  0],\n",
    "       [ 0,  0,  0, k0, k1, k2],\n",
    "       [ 0,  0,  0, k3, k4, k5]]\n",
    "```\n",
    "\n",
    "Now we calculate the vector-Jacobian product:\n",
    "\n",
    "```python\n",
    "flatten(gX) = [gx0, gx1, gx2, gx3]\n",
    "\n",
    "vjp_Q = flatten(gX) * J_Q\n",
    "vjp_Q = [gx0*k0 + gx1*k3,\n",
    "         gx0*k1 + gx1*k4,\n",
    "         gx0*k2 + gx1*k5,\n",
    "         gx2*k0 + gx3*k3,\n",
    "         gx2*k1 + gx3*k4,\n",
    "         gx2*k2 + gx3*k5]\n",
    "```\n",
    "\n",
    "Finally, we get `gQ` by unflattening to match the shape of `Q`, (2, 3):\n",
    "\n",
    "```python\n",
    "gQ = [[(gx0*k0 + gx1*k3), (gx0*k1 + gx1*k4), (gx0*k2 + gx1*k5)],\n",
    "      [(gx2*k0 + gx3*k3), (gx2*k1 + gx3*k4), (gx2*k2 + gx3*k5)]]\n",
    "```\n",
    "\n",
    "So we can see that the result of `gQ` is actually just some operation between `gX` and `Q`. I'll copy `gX` and `K` here to make it easier to see a pattern.\n",
    "\n",
    "```python\n",
    "gX = [[gx0, gx1],\n",
    "      [gx2, gx3]]\n",
    "\n",
    "K = [[k0, k1, k2],\n",
    "     [k3, k4, k5]]\n",
    "```\n",
    "\n",
    "Okay, easy! This is just a matrix multiplication, `gQ = matmul(gX, K)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e4b0d",
   "metadata": {},
   "source": [
    "## Implementing the matrix multiply gradient\n",
    "\n",
    "We are given some boilerplate code plug our solution into. It contains code to test it for correctness.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "class Attn(torch.autograd.Function):\n",
    "    # Start with matmul\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k):\n",
    "        x = torch.matmul(q, k.transpose(0, 1))\n",
    "        ctx.save_for_backward(...)\n",
    "        return x\n",
    "\n",
    "    # Go for the kill\n",
    "    # @staticmethod\n",
    "    # def forward(ctx, q, k, v):\n",
    "    #     x = torch.matmul(q, k.transpose(0, 1))\n",
    "    #     a = torch.tanh(x)\n",
    "    #     o = torch.matmul(a, v)\n",
    "    #     ctx.save_for_backward(...)\n",
    "    #     return o, a\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gx):\n",
    "        q, k, ... = ctx.saved_tensors\n",
    "        ...\n",
    "        return gq, gk\n",
    "\n",
    "q = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "k = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "v = torch.rand(2, 4, dtype=torch.float64, requires_grad=True)\n",
    "attn = Attn.apply\n",
    "\n",
    "torch.autograd.gradcheck(attn, (q, k, v), atol=0.01)\n",
    "```\n",
    "\n",
    "So I plugged my backward formula for `gQ` into this code and removed all the extra stuff I don't need yet.\n",
    "\n",
    "After I verified that the `gQ` formula worked, I also figured out the formula for `gK`. I didn't actually go through the calculations I did for `gQ`. I figured that the two formulas would have to be almost the same as each other. But for the `gK` formula, I thought that `Q` would have to be transposed since `K` is transposed in the forward function. Then, I figured that we would have to matrix multiply with `gX`, since we did that for `gQ`. I saw that the only ordering that works is `matmul(Q^T, gX)`, since the sizes wouldn't match up correctly if I did `matmul(gX, Q^T)`. You can't multiply a 2x2 matrix by a 3x2 matrix, but you can multiply a 3x2 matrix by a 2x2 matrix. The output of the multiply is 3x2, but I needed 2x3, so I figured I'd just transpose it and see if it works. And it did! The final formula is `gK = matmul(Q^T, gX)^T`. It would also be possible to go through the whole Jacobian-vector product calculation to get there, but why bother if you can avoid it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1ae7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/endoplasm/miniconda3/envs/main/lib/python3.9/site-packages/torch/autograd/__init__.py:234: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811803361/work/c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Attn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K):\n",
    "        X = torch.matmul(Q, K.transpose(0, 1))\n",
    "        ctx.save_for_backward(Q, K)\n",
    "        return X\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gX):\n",
    "        Q, K = ctx.saved_tensors\n",
    "        \n",
    "        gQ = torch.matmul(gX, K)\n",
    "        gK = torch.matmul(Q.t(), gX).t()\n",
    "        \n",
    "        return gQ, gK\n",
    "\n",
    "Q = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "K = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "attn = Attn.apply\n",
    "\n",
    "torch.autograd.gradcheck(attn, (Q, K), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51cf3c",
   "metadata": {},
   "source": [
    "## Deriving the `tanh` gradient\n",
    "\n",
    "The next part of the assignment is to compute the gradient for the operation `A = tanh(X)`. Recall that in our example above, `X` has the shape `(2, 2)`. Since `tanh` is a pointwise operation, the output will also be `(2, 2)`. We will assume that we're given a gradient for the output `gA` of shape equal to that of `A`, and we have to calculuate `gX`.\n",
    "\n",
    "```python\n",
    "X = [[x0, x1],\n",
    "     [x2, x3]]\n",
    "\n",
    "A = [[a0, a1],\n",
    "     [a2, a3]]\n",
    "A = [[tanh(x0), tanh(x1)],\n",
    "     [tanh(x2), tanh(x3)]]\n",
    "\n",
    "J_X = [[∂a0/∂x0, ∂a0/∂x1, ∂a0/∂x2, ∂a0/∂x3],\n",
    "       [∂a1/∂x0, ∂a1/∂x1, ∂a1/∂x2, ∂a1/∂x3],\n",
    "       [∂a2/∂x0, ∂a2/∂x1, ∂a2/∂x2, ∂a2/∂x3],\n",
    "       [∂a3/∂x0, ∂a3/∂x1, ∂a3/∂x2, ∂a3/∂x3]]\n",
    "```\n",
    "\n",
    "To calculate these partial derivatives, we'll have to use the fact that for a scalar `x`,\n",
    "\n",
    "```python\n",
    "d (tanh(x)) / dx = 1 - tanh(x)^2\n",
    "```\n",
    "\n",
    "Substituting all the partial derivatives in `J_X`:\n",
    "\n",
    "```python\n",
    "J_X = [[1-tanh(x0)^2,            0,            0,            0],\n",
    "       [           0, 1-tanh(x1)^2,            0,            0],\n",
    "       [           0,            0, 1-tanh(x2)^2,            0],\n",
    "       [           0,            0,            0, 1-tanh(x3)^2]]\n",
    "```\n",
    "\n",
    "Now we can calculate the Jacobian-vector product between `flatten(gA)` and `J_X`:\n",
    "\n",
    "```python\n",
    "flatten(gA) = [ga0, ga1, ga2, ga3]\n",
    "vjp_X = flatten(gA) * J_X\n",
    "vjp_X = [\n",
    "    ga0 * (1 - tanh(x0)^2),\n",
    "    ga1 * (1 - tanh(x1)^2),\n",
    "    ga2 * (1 - tanh(x2)^2),\n",
    "    ga3 * (1 - tanh(x3)^2)]\n",
    "```\n",
    "\n",
    "Then we reshape this to the shape of `X`, `(2, 2)`, to get `gX`:\n",
    "\n",
    "```python\n",
    "gX = [[ga0 * (1 - tanh(x0)^2), ga1 * (1 - tanh(x1)^2)],\n",
    "      [ga2 * (1 - tanh(x2)^2), ga3 * (1 - tanh(x3)^2)]]\n",
    "```\n",
    "\n",
    "We can see that this is actually the same as `gX = gA * (1 - tanh(X)^2)`\n",
    "\n",
    "Let's verify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f44bcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Attn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X):\n",
    "        A = torch.tanh(X)\n",
    "        ctx.save_for_backward(X)\n",
    "        return A\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gA):\n",
    "        (X,) = ctx.saved_tensors\n",
    "        \n",
    "        gX = gA * (1 - torch.tanh(X)**2)\n",
    "        \n",
    "        return gX\n",
    "\n",
    "X = torch.rand(2, 2, dtype=torch.float64, requires_grad=True)\n",
    "attn = Attn.apply\n",
    "\n",
    "torch.autograd.gradcheck(attn, (X,), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa4255",
   "metadata": {},
   "source": [
    "## Another matmul\n",
    "\n",
    "The last operation is `O = matmul(A, V)`. This is slightly different than the matmul gradient we calculated before, since `V` is not transposed.\n",
    "\n",
    "If `A` is shape `(2, 2)` and `V` is shape `(2, 4)` then `O` will be `(2, 4)`. We're given `gO`, which will be `(2, 4)` in this example.\n",
    "\n",
    "Of course, we can reuse our previous work for this example. Remember that we showed earlier that `gQ = matmul(gX, K)` and `gK = matmul(Q^T, gX)^T` for the operation `X = matmul(Q, K^T)`. We can perform some substitutions here, `X --> O`, `Q --> A`, and `K^T --> V`.\n",
    "\n",
    "```python\n",
    "gQ = matmul(gX, K)\n",
    "gK = matmul(Q^T, gX)^T\n",
    "```\n",
    "\n",
    "After substitution:\n",
    "\n",
    "```python\n",
    "gA = matmul(gO, V^T)\n",
    "\n",
    "gV^T = matmul(A^T, gO)^T\n",
    "gV = matmul(A^T, gO)\n",
    "\n",
    "```\n",
    "\n",
    "And now let's test the formulae for `gA` and `gV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7e39fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Attn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, A, V):\n",
    "        O = torch.matmul(A, V)\n",
    "        ctx.save_for_backward(A, V)\n",
    "        return O\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gO):\n",
    "        A, V = ctx.saved_tensors\n",
    "        \n",
    "        gA = torch.matmul(gO, V.t())\n",
    "        gV = torch.matmul(A.t(), gO)\n",
    "        \n",
    "        return gA, gV\n",
    "\n",
    "A = torch.rand(2, 2, dtype=torch.float64, requires_grad=True)\n",
    "V = torch.rand(2, 4, dtype=torch.float64, requires_grad=True)\n",
    "attn = Attn.apply\n",
    "\n",
    "torch.autograd.gradcheck(attn, (A, V), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866edbf7",
   "metadata": {},
   "source": [
    "## Combining everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e47e5d",
   "metadata": {},
   "source": [
    "Now we have to combine all of gradient formulae into one. Since gradient calculations flow backwards, we'll put them in the opposite order of how they appear in the forward function. Here's the forward again:\n",
    "\n",
    "```python\n",
    "X = matmul(Q, Q^T)\n",
    "A = tanh(X)\n",
    "O = matmul(A, V)\n",
    "return O, A\n",
    "```\n",
    "\n",
    "Here's how we'll lay out the backward function:\n",
    "\n",
    "```python\n",
    "# Gradients for second matrix multiply `O = matmul(A, V)`:\n",
    "gA = ...\n",
    "gV = ...\n",
    "\n",
    "# Gradient for `A = tanh(X)`\n",
    "gX = ...\n",
    "\n",
    "# Gradients for first matrix multiply `X = matmul(Q, Q^T)`:\n",
    "gQ = ...\n",
    "gK = ...\n",
    "```\n",
    "\n",
    "Remember, we're returning both `O` and `A` from the forward function, so the backward function will receive `gO` and `gA` as arguments. This seems kind of odd--we'll receive a `gA`, but we also have to calculate a `gA` for the second matrix multiply from the forward function. So we have two different gradients for `A`. What are we supposed to do with them?\n",
    "\n",
    "To answer that question, let's imagine a more general case. Let's imagine we have the following forward function:\n",
    "\n",
    "```python\n",
    "Z = f0(Y)\n",
    "W = f1(Y)\n",
    "return Z, W\n",
    "```\n",
    "\n",
    "Here, `f0` and `f1` are arbitrary operations. `Y` is the input and `Z` and `W` are the outputs of the forward function. We'll be given `gZ` and `gW` for the backward formula, and we'll need to calculate `gY`. Here's the layout of the backward function:\n",
    "\n",
    "```python\n",
    "gY1 = f1_backward(gW, Y)\n",
    "gY0 = f0_backward(gZ, Y)\n",
    "gY = ???\n",
    "```\n",
    "\n",
    "Since we used `Y` as the input to two different operations that have their own separate differentiable outputs, we have to calculate two different gradients for `Y`, `gY0` and `gY1` above. Then we have to combine them somehow to get the full gradient `gY`. How do we combine them?\n",
    "\n",
    "Well, let's think about what happens if we change `gW` a little bit. `gY1` will change a little bit and `gY0` will remain unchanged. `gY` needs to change a little bit as well, and the amount that it changes by has to be equal to the amount that `gY1` changed.\n",
    "\n",
    "On the other hand, if we changed `gZ` a little bit, `gY0` would have to change a little bit, `gY1` would stay unchanged, and `gY` will change by the same amount that `gY0` changed by.\n",
    "\n",
    "So `gY` needs to be exactly proportional to both `gY0` and `gY1`. So it turns out that the answer is to just add `gY0` and `gY1` together to get `gY`. (I don't know a more rigorous explanation than this. It would be good to find an article that explains it and link it here.)\n",
    "\n",
    "```python\n",
    "gY = gY0 + gY1\n",
    "```\n",
    "\n",
    "Using the same logic for the backward formula for our activation function, we'll just add together our two different gradients for `A` and use the result for the `gX` formula. Now let's put everything together:\n",
    "\n",
    "```python\n",
    "# Given gO and gA\n",
    "\n",
    "# Gradients for second matrix multiply `O = matmul(A, V)`:\n",
    "gA_combined = gA + matmul(gO, V^T)\n",
    "gV = matmul(A^T, gO)\n",
    "\n",
    "# Gradient for `A = tanh(X)`\n",
    "gX = gA_combined * (1 - tanh(x)^2)\n",
    "\n",
    "# Gradients for first matrix multiply `X = matmul(Q, Q^T)`:\n",
    "gQ = matmul(gX, K)\n",
    "gK = matmul(Q^T, gX)^T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "245bd53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Attn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, Q, K, V):\n",
    "        X = torch.matmul(Q, K.transpose(0, 1))\n",
    "        A = torch.tanh(X)\n",
    "        O = torch.matmul(A, V)\n",
    "        \n",
    "        ctx.save_for_backward(Q, K, V, X, A)\n",
    "        return O, A\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, gO, gA):\n",
    "        Q, K, V, X, A = ctx.saved_tensors\n",
    "\n",
    "        gA_combined = gA + torch.matmul(gO, V.t())\n",
    "        gV = torch.matmul(A.t(), gO)\n",
    "        gX = gA_combined * (1 - torch.tanh(X)**2)\n",
    "        gQ = torch.matmul(gX, K)\n",
    "        gK = torch.matmul(Q.t(), gX).t()\n",
    "        \n",
    "        return gQ, gK, gV\n",
    "\n",
    "Q = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "K = torch.rand(2, 3, dtype=torch.float64, requires_grad=True)\n",
    "V = torch.rand(2, 4, dtype=torch.float64, requires_grad=True)\n",
    "attn = Attn.apply\n",
    "\n",
    "torch.autograd.gradcheck(attn, (Q, K, V), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d608fc6",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ed83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
